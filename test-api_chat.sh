curl -XPOST 127.0.0.1:8000/api/chat -H "Content-Type: application/json" --data '{"ask": "and how many of these are unhealthy?", "conversation_history": [{"role": "system", "content": "You are a tool-calling AI assist provided with common devops and IT tools that you can use to troubleshoot problems or answer questions.\nWhenever possible you MUST first use tools to investigate then answer the question.\nDo not say - based on the tool output - or explicitly refer to tools at all.\nIf you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.\nIf you have a good and concrete suggestion for how the user can fix something, tell them even if not asked explicitly\n\nUse conversation history to maintain continuity when appropriate, ensuring efficiency in your responses.\n\n\nIn general:\n* when it can provide extra information, first run as many tools as you need to gather more information, then respond.\n* if possible, do so repeatedly with different tool calls each time to gather more information.\n* do not stop investigating until you are at the final root cause you are able to find.\n* use the \"five whys\" methodology to find the root cause.\n* for example, if you found a problem in microservice A that is due to an error in microservice B, look at microservice B too and find the error in that.\n* if you cannot find the resource/application that the user referred to, assume they made a typo or included/excluded characters like - and.\n* in this case, try to find substrings or search for the correct spellings\n* always provide detailed information like exact resource names, versions, labels, etc\n* even if you found the root cause, keep investigating to find other possible root causes and to gather data for the answer like exact names\n* when giving an answer don t say root cause but \"possible root causes\" and be clear to distinguish between what you know for certain and what is a possible explanation\n* if a runbook url is present as well as tool that can fetch it, you MUST fetch the runbook before beginning your investigation.\n* if you don t know, say that the analysis was inconclusive.\n* if there are multiple possible causes list them in a numbered list.\n* there will often be errors in the data that are not relevant or that do not have an impact - ignore them in your conclusion if you were not able to tie them to an actual error.\n\nIf investigating Kubernetes problems:\n* run as many kubectl commands as you need to gather more information, then respond.\n* if possible, do so repeatedly on different Kubernetes objects.\n* for example, for deployments first run kubectl on the deployment then a replicaset inside it, then a pod inside that.\n* if the user wants to find a specific term in a pod s logs, use kubectl_logs_grep\n* use both kubectl_previous_logs and kubectl_logs when reading logs. Treat the output of both as a single unified logs stream\n* when investigating a pod that crashed or application errors, always run kubectl_describe and fetch the logs\n* do not give an answer like \"The pod is pending\" as that doesn t state why the pod is pending and how to fix it.\n* do not give an answer like \"Pod s node affinity/selector doesn t match any available nodes\" because that doesn t include data on WHICH label doesn t match\n* if investigating an issue on many pods, there is no need to check more than 3 individual pods in the same deployment. pick up to a representative 3 from each deployment if relevant\n* if the user says something isn t working, ALWAYS:\n** use kubectl_describe on the owner workload + individual pods and look for any transient issues they might have been referring to\n** check the application aspects through the logs (kubectl_logs and kubectl_previous_logs) and other relevant tools\n** look for misconfigured ingresses/services etc\n\nHandling Permission Errors\nIf during the investigation you encounter a permissions error (e.g., `Error from server (Forbidden):`), **ALWAYS** follow these steps to ensure a thorough resolution:\n1.**Analyze the Error Message**\n - Identify the missing resource, API group, and verbs from the error details.\n - Never stop at reporting the error\n - Proceed with an in-depth investigation.\n2.**Locate the Relevant Helm Release**\nCheck if Helm tools are available, if they are available always use Helm commands to help user find the release associated with the Holmes pod:\n - Run `helm list -A | grep holmes` to identify the release name.\n - Run `helm get values <RELEASE_NAME> -n <NAMESPACE>` to retrieve details such as `customClusterRoleRules` and `clusterName`.\nIf Helm tools are unavailable, skip this step.\n3. **Check for Missing Permissions**\n - Check for a cluster role with <RELEASE_NAME>-holmes-cluster-role in its name and a service account with <RELEASE_NAME>-holmes-service-account in its name to troubleshoot missing permissions where release name is the name you found earlier if helm tools are available (If the exact cluster role or service account isn t found, search for similar or related names, including variations or prefixes/suffixes that might be used in the cluster.)\n - Focus on identifying absent permissions that align with the error message.\n4. **Update the Configuration**\nIf necessary permissions are absent both in customClusterRoleRules and the cluster role mentioned previously, ALWAYS advise the user to update their configuration by modifying the `generated_values.yaml` file as follows: \n```\nholmes:\n    customClusterRoleRules:\n      - apiGroups: [\"<API_GROUP>\"]\n        resources: [\"<RESOURCE_1>\", \"<RESOURCE_2>\"]\n        verbs: [\"<VERB_1>\", \"<VERB_2>\", \"<VERB_3>\"]\n```\nAfter that instruct them to apply the changes with::\n```\n    helm upgrade <RELEASE_NAME> robusta/robusta --values=generated_values.yaml --set clusterName=<YOUR_CLUSTER_NAME>\n```\n5. **Fallback Guidelines**\n- If you cannot determine the release or cluster name, use placeholders `<RELEASE_NAME>` and `<YOUR_CLUSTER_NAME>`.\n- While you should attempt to retrieve details using Helm commands, do **not** direct the user to execute these commands themselves.\nReminder:\n* Always adhere to this process, even if Helm tools are unavailable.\n* Strive for thoroughness and precision, ensuring the issue is fully addressed.\n\nSpecial cases and how to reply:\n* if you are unable to investigate something properly because you do not have tools that let you access the right data, explicitly tell the user that you are missing an integration to access XYZ which you would need to investigate. you should give an answer similar to \"I don t have access to <details>. Please add a Holmes integration for <XYZ> so that I can investigate this.\"\n* make sure you differentiate between \"I investigated and found error X caused this problem\" and \"I tried to investigate but while investigating I got some errors that prevented me from completing the investigation.\"\n* as a special case of that, If a tool generates a permission error when attempting to run it, follow the Handling Permission Errors section for detailed guidance.\n* that is different than - for example - fetching a pod s logs and seeing that the pod itself has permission errors. in that case, you explain say that permission errors are the cause of the problem and give details\n* Issues are a subset of findings. When asked about an issue or a finding and you have an id, use the tool `fetch_finding_by_id`.\n* For any question, try to make the answer specific to the user s cluster.\n** For example, if asked to port forward, find out the app or pod port (kubectl decribe) and provide a port forward command specific to the user s question\n\n\nStyle guide:\n* Reply with terse output.\n* Be painfully concise.\n* Leave out \"the\" and filler words when possible.\n* Be terse but not at the expense of leaving out important data like the root cause and how to fix.\n\nExamples:\n\nUser: Why did the webserver-example app crash?\n(Call tool kubectl_find_resource kind=pod keyword=webserver`)\n(Call tool kubectl_previous_logs namespace=demos pod=webserver-example-1299492-d9g9d # this pod name was found from the previous tool call)\n\nAI: `webserver-example-1299492-d9g9d` crashed due to email validation error during HTTP request for /api/create_user\nRelevant logs:\n\n```\n2021-01-01T00:00:00.000Z [ERROR] Missing required field email in request body\n```\n\nValidation error led to unhandled Java exception causing a crash."}, {"role": "user", "content": "Hi"}, {"content": "Hello! How can I assist you today?", "role": "assistant"}, {"role": "user", "content": "How many pods are running?"}, {"role": "assistant", "tool_calls": [{"function": {"arguments": "{\"kind\":\"pod\"}", "name": "kubectl_get_by_kind_in_cluster"}, "id": "call_ZMgO2X6GrMVF16YbLW02lHZW", "type": "function"}]}, {"tool_call_id": "call_ZMgO2X6GrMVF16YbLW02lHZW", "role": "tool", "name": "kubectl_get_by_kind_in_cluster", "content": "stdout:\nNAMESPACE            NAME                                                         READY   STATUS    RESTARTS        AGE     IP            NODE                          NOMINATED NODE   READINESS GATES   LABELS\nargocd               argocd-application-controller-0                              1/1     Running   9 (111m ago)    29d     10.244.0.12   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-application-controller,apps.kubernetes.io/pod-index=0,controller-revision-hash=argocd-application-controller-77d8549fd,statefulset.kubernetes.io/pod-name=argocd-application-controller-0\nargocd               argocd-applicationset-controller-7ff94fc879-lfrcp            1/1     Running   9 (111m ago)    29d     10.244.0.5    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-applicationset-controller,pod-template-hash=7ff94fc879\nargocd               argocd-dex-server-84b879d87c-xpwvh                           1/1     Running   9 (111m ago)    29d     10.244.0.17   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-dex-server,pod-template-hash=84b879d87c\nargocd               argocd-notifications-controller-6c65b4b9f6-5z2nn             1/1     Running   9 (111m ago)    29d     10.244.0.7    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-notifications-controller,pod-template-hash=6c65b4b9f6\nargocd               argocd-redis-868dbb7cf4-kwjg2                                1/1     Running   9 (111m ago)    29d     10.244.0.6    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-redis,pod-template-hash=868dbb7cf4\nargocd               argocd-repo-server-6d47848766-9l5kf                          1/1     Running   9 (111m ago)    29d     10.244.0.16   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-repo-server,pod-template-hash=6d47848766\nargocd               argocd-server-c9f58d8cf-5wz2j                                1/1     Running   16 (111m ago)   29d     10.244.0.15   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/name=argocd-server,pod-template-hash=c9f58d8cf\ndefault              alertmanager-robusta-kube-prometheus-st-alertmanager-0       2/2     Running   4 (111m ago)    2d17h   10.244.0.24   grafana-cloud-control-plane   <none>           <none>            alertmanager=robusta-kube-prometheus-st-alertmanager,app.kubernetes.io/instance=robusta-kube-prometheus-st-alertmanager,app.kubernetes.io/managed-by=prometheus-operator,app.kubernetes.io/name=alertmanager,app.kubernetes.io/version=0.26.0,apps.kubernetes.io/pod-index=0,controller-revision-hash=alertmanager-robusta-kube-prometheus-st-alertmanager-57cd7fb46f,statefulset.kubernetes.io/pod-name=alertmanager-robusta-kube-prometheus-st-alertmanager-0\ndefault              customer-orders-766b65899b-hfpss                             2/2     Running   18 (111m ago)   30d     10.244.0.4    grafana-cloud-control-plane   <none>           <none>            app=customer-orders,pod-template-hash=766b65899b\ndefault              grafana-k8s-monitoring-alloy-0                               2/2     Running   21 (110m ago)   30d     10.244.0.2    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=grafana-k8s-monitoring,app.kubernetes.io/name=alloy,apps.kubernetes.io/pod-index=0,controller-revision-hash=grafana-k8s-monitoring-alloy-7cf58b875b,statefulset.kubernetes.io/pod-name=grafana-k8s-monitoring-alloy-0\ndefault              grafana-k8s-monitoring-alloy-events-799cc88c88-9pllx         2/2     Running   22 (110m ago)   30d     10.244.0.3    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=grafana-k8s-monitoring,app.kubernetes.io/name=alloy-events,pod-template-hash=799cc88c88\ndefault              grafana-k8s-monitoring-alloy-logs-t4ktj                      2/2     Running   22 (110m ago)   30d     10.244.0.13   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=grafana-k8s-monitoring,app.kubernetes.io/name=alloy-logs,controller-revision-hash=fdd8b877b,pod-template-generation=1\ndefault              grafana-k8s-monitoring-kepler-2hntq                          1/1     Running   9 (111m ago)    30d     172.18.0.3    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=exporter,app.kubernetes.io/name=kepler,controller-revision-hash=7b459fdb69,pod-template-generation=1\ndefault              grafana-k8s-monitoring-kube-state-metrics-5ff5b4947b-b7l57   1/1     Running   20 (110m ago)   30d     10.244.0.8    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=metrics,app.kubernetes.io/instance=grafana-k8s-monitoring,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=kube-state-metrics,app.kubernetes.io/part-of=kube-state-metrics,app.kubernetes.io/version=2.14.0,helm.sh/chart=kube-state-metrics-5.27.0,pod-template-hash=5ff5b4947b,release=grafana-k8s-monitoring\ndefault              grafana-k8s-monitoring-opencost-5b67d55db5-nwp7l             1/1     Running   9 (111m ago)    30d     10.244.0.14   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=grafana-k8s-monitoring,app.kubernetes.io/name=opencost,pod-template-hash=5b67d55db5\ndefault              grafana-k8s-monitoring-prometheus-node-exporter-wjb7d        1/1     Running   9 (111m ago)    30d     172.18.0.3    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=metrics,app.kubernetes.io/instance=grafana-k8s-monitoring,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/part-of=prometheus-node-exporter,app.kubernetes.io/version=1.8.2,controller-revision-hash=869855d879,helm.sh/chart=prometheus-node-exporter-4.42.0,pod-template-generation=1,release=grafana-k8s-monitoring\ndefault              kafka-client                                                 0/1     Unknown   0               28d     <none>        grafana-cloud-control-plane   <none>           <none>            run=kafka-client\ndefault              kafka-consumer                                               0/1     Unknown   0               28d     <none>        grafana-cloud-control-plane   <none>           <none>            run=kafka-consumer\ndefault              kafka-controller-0                                           1/1     Running   18 (111m ago)   28d     10.244.0.26   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=controller-eligible,app.kubernetes.io/instance=kafka,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=kafka,app.kubernetes.io/part-of=kafka,app.kubernetes.io/version=3.9.0,apps.kubernetes.io/pod-index=0,controller-revision-hash=kafka-controller-58645b745b,helm.sh/chart=kafka-31.1.0,statefulset.kubernetes.io/pod-name=kafka-controller-0\ndefault              kafka-controller-1                                           1/1     Running   16 (111m ago)   28d     10.244.0.23   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=controller-eligible,app.kubernetes.io/instance=kafka,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=kafka,app.kubernetes.io/part-of=kafka,app.kubernetes.io/version=3.9.0,apps.kubernetes.io/pod-index=1,controller-revision-hash=kafka-controller-58645b745b,helm.sh/chart=kafka-31.1.0,statefulset.kubernetes.io/pod-name=kafka-controller-1\ndefault              kafka-controller-2                                           1/1     Running   19 (111m ago)   28d     10.244.0.28   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=controller-eligible,app.kubernetes.io/instance=kafka,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=kafka,app.kubernetes.io/part-of=kafka,app.kubernetes.io/version=3.9.0,apps.kubernetes.io/pod-index=2,controller-revision-hash=kafka-controller-58645b745b,helm.sh/chart=kafka-31.1.0,statefulset.kubernetes.io/pod-name=kafka-controller-2\ndefault              kafka-producer                                               0/1     Unknown   0               28d     <none>        grafana-cloud-control-plane   <none>           <none>            run=kafka-producer\ndefault              prometheus-robusta-kube-prometheus-st-prometheus-0           2/2     Running   4 (111m ago)    2d17h   10.244.0.27   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=robusta-kube-prometheus-st-prometheus,app.kubernetes.io/managed-by=prometheus-operator,app.kubernetes.io/name=prometheus,app.kubernetes.io/version=2.48.1,apps.kubernetes.io/pod-index=0,controller-revision-hash=prometheus-robusta-kube-prometheus-st-prometheus-55d87c869b,operator.prometheus.io/name=robusta-kube-prometheus-st-prometheus,operator.prometheus.io/shard=0,prometheus=robusta-kube-prometheus-st-prometheus,statefulset.kubernetes.io/pod-name=prometheus-robusta-kube-prometheus-st-prometheus-0\ndefault              robusta-forwarder-5c5fdbbf57-z6sxp                           1/1     Running   2 (111m ago)    2d17h   10.244.0.21   grafana-cloud-control-plane   <none>           <none>            app=robusta-forwarder,pod-template-hash=5c5fdbbf57\ndefault              robusta-grafana-8588b8fb85-dw8rn                             3/3     Running   6 (111m ago)    2d17h   10.244.0.20   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=robusta,app.kubernetes.io/name=grafana,pod-template-hash=8588b8fb85\ndefault              robusta-holmes-775d7ddcb9-f4jxg                              1/1     Running   0               50s     10.244.0.49   grafana-cloud-control-plane   <none>           <none>            app=holmes,pod-template-hash=775d7ddcb9\ndefault              robusta-kube-prometheus-st-operator-6885c8f675-7wqpd         1/1     Running   5 (110m ago)    2d17h   10.244.0.18   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/instance=robusta,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/part-of=kube-prometheus-stack,app.kubernetes.io/version=55.7.0,app=kube-prometheus-stack-operator,chart=kube-prometheus-stack-55.7.0,heritage=Helm,pod-template-hash=6885c8f675,release=robusta\ndefault              robusta-kube-state-metrics-8667fd9775-7dnqk                  1/1     Running   6 (110m ago)    2d17h   10.244.0.19   grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=metrics,app.kubernetes.io/instance=robusta,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=kube-state-metrics,app.kubernetes.io/part-of=kube-state-metrics,app.kubernetes.io/version=2.10.1,helm.sh/chart=kube-state-metrics-5.15.3,pod-template-hash=8667fd9775,release=robusta\ndefault              robusta-prometheus-node-exporter-9dczn                       1/1     Running   2 (111m ago)    2d17h   172.18.0.3    grafana-cloud-control-plane   <none>           <none>            app.kubernetes.io/component=metrics,app.kubernetes.io/instance=robusta,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/part-of=prometheus-node-exporter,app.kubernetes.io/version=1.7.0,controller-revision-hash=7b4f94f6bf,helm.sh/chart=prometheus-node-exporter-4.24.0,jobLabel=node-exporter,pod-template-generation=1,release=robusta\ndefault              robusta-runner-64c4d84845-p725f                              1/1     Running   1 (111m ago)    43h     10.244.0.22   grafana-cloud-control-plane   <none>           <none>            app=robusta-runner,pod-template-hash=64c4d84845,robustaComponent=runner\nkube-system          coredns-7c65d6cfc9-h65ff                                     1/1     Running   9 (111m ago)    30d     10.244.0.10   grafana-cloud-control-plane   <none>           <none>            k8s-app=kube-dns,pod-template-hash=7c65d6cfc9\nkube-system          coredns-7c65d6cfc9-v6lvm                                     1/1     Running   9 (111m ago)    30d     10.244.0.11   grafana-cloud-control-plane   <none>           <none>            k8s-app=kube-dns,pod-template-hash=7c65d6cfc9\nkube-system          etcd-grafana-cloud-control-plane                             1/1     Running   0               111m    172.18.0.3    grafana-cloud-control-plane   <none>           <none>            component=etcd,tier=control-plane\nkube-system          kindnet-x9k26                                                1/1     Running   9 (111m ago)    30d     172.18.0.3    grafana-cloud-control-plane   <none>           <none>            app=kindnet,controller-revision-hash=85f8898dd9,k8s-app=kindnet,pod-template-generation=1,tier=node\nkube-system          kube-apiserver-grafana-cloud-control-plane                   1/1     Running   0               111m    172.18.0.3    grafana-cloud-control-plane   <none>           <none>            component=kube-apiserver,tier=control-plane\nkube-system          kube-controller-manager-grafana-cloud-control-plane          1/1     Running   10 (111m ago)   30d     172.18.0.3    grafana-cloud-control-plane   <none>           <none>            component=kube-controller-manager,tier=control-plane\nkube-system          kube-proxy-s6qsc                                             1/1     Running   13 (111m ago)   30d     172.18.0.3    grafana-cloud-control-plane   <none>           <none>            controller-revision-hash=77987969cc,k8s-app=kube-proxy,pod-template-generation=1\nkube-system          kube-scheduler-grafana-cloud-control-plane                   1/1     Running   10 (111m ago)   30d     172.18.0.3    grafana-cloud-control-plane   <none>           <none>            component=kube-scheduler,tier=control-plane\nlocal-path-storage   local-path-provisioner-57c5987fd4-cr8dx                      1/1     Running   18 (110m ago)   30d     10.244.0.9    grafana-cloud-control-plane   <none>           <none>            app=local-path-provisioner,pod-template-hash=57c5987fd4\n\nstderr:\n"}, {"content": "There are 36 pods currently running in the cluster.", "role": "assistant"}]}'
"
